{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2c67de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TTNT1\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda - Falcon-1B_FTT_SST-2.ipynb:30\n",
      "GPU Name: NVIDIA RTX A5000 - Falcon-1B_FTT_SST-2.ipynb:32\n",
      "VRAM: 25.76 GB - Falcon-1B_FTT_SST-2.ipynb:33\n",
      "Loading SST2 (Stanford Sentiment Treebank) - Falcon-1B_FTT_SST-2.ipynb:36\n",
      ">>> Dataset Loaded Successfully! - Falcon-1B_FTT_SST-2.ipynb:38\n"
     ]
    }
   ],
   "source": [
    "# @title Phần 1: Setup Environment & Load SST-2\n",
    "# !pip install -q transformers datasets evaluate scikit-learn accelerate psutil\n",
    "\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# 1. Setup Path (Nơi lưu kết quả)\n",
    "SAVE_PATH = './Falcon_1B_SST2_Results'\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "# 2. Check Device & GPU (RTX A5000)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} - Falcon-1B_FTT_SST-2.ipynb:30\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)} - Falcon-1B_FTT_SST-2.ipynb:32\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB - Falcon-1B_FTT_SST-2.ipynb:33\")\n",
    "\n",
    "# 3. Load SST-2 Dataset\n",
    "print(\"Loading SST2 (Stanford Sentiment Treebank) - Falcon-1B_FTT_SST-2.ipynb:36\")\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "print(\">>> Dataset Loaded Successfully! - Falcon-1B_FTT_SST-2.ipynb:38\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ff687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer: tiiuae/falcon-rw-1b... - Falcon-1B_FTT_SST-2.ipynb:4\n",
      "Tokenizing Dataset... - Falcon-1B_FTT_SST-2.ipynb:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 872/872 [00:00<00:00, 7464.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Tokenization Complete. Train size: 67349, Val size: 872 - Falcon-1B_FTT_SST-2.ipynb:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Phần 2: Tokenization (Falcon-1B)\n",
    "MODEL_NAME = 'tiiuae/falcon-rw-1b'\n",
    "\n",
    "print(f\"Loading Tokenizer: {MODEL_NAME}... - Falcon-1B_FTT_SST-2.ipynb:4\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# LƯU Ý QUAN TRỌNG: Falcon không có pad_token mặc định -> Gán bằng eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # SST-2 sử dụng cột 'sentence'\n",
    "    # Giữ max_length=512 để đồng nhất benchmark với IMDB\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing Dataset... - Falcon-1B_FTT_SST-2.ipynb:20\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Chuẩn hóa tên cột cho PyTorch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Chia tập train/val\n",
    "dataset_train = tokenized_datasets[\"train\"]\n",
    "dataset_val = tokenized_datasets[\"validation\"]\n",
    "\n",
    "print(f\">>> Tokenization Complete. Train size: {len(dataset_train)}, Val size: {len(dataset_val)} - Falcon-1B_FTT_SST-2.ipynb:32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4928fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: tiiuae/falcon-rw-1b in BFloat16... - Falcon-1B_FTT_SST-2.ipynb:7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-rw-1b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loaded successfully using SafeTensors (BF16). - Falcon-1B_FTT_SST-2.ipynb:16\n",
      ">>> Model Ready on GPU! - Falcon-1B_FTT_SST-2.ipynb:31\n"
     ]
    }
   ],
   "source": [
    "# @title Phần 3: Load Model (BF16 Optimized)\n",
    "\n",
    "# Dọn dẹp VRAM trước khi load\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loading Model: {MODEL_NAME} in BFloat16... - Falcon-1B_FTT_SST-2.ipynb:7\")\n",
    "try:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        # A5000 hỗ trợ Bfloat16 -> Chạy nhanh & ổn định hơn FP16\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    print(\">>> Loaded successfully using SafeTensors (BF16). - Falcon-1B_FTT_SST-2.ipynb:16\")\n",
    "except Exception as e:\n",
    "    print(f\">>> SafeTensors failed ({e}). Loading legacy mode... - Falcon-1B_FTT_SST-2.ipynb:18\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_safetensors=False\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Bật Gradient Checkpointing để tiết kiệm VRAM nếu cần\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\">>> Model Ready on GPU! - Falcon-1B_FTT_SST-2.ipynb:31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58aaf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training (Falcon1B on SST2 | Turbo Mode BF16) - Falcon-1B_FTT_SST-2.ipynb:65\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6315' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6315/6315 1:29:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.126200</td>\n",
       "      <td>0.131821</td>\n",
       "      <td>0.958716</td>\n",
       "      <td>0.959821</td>\n",
       "      <td>0.951327</td>\n",
       "      <td>0.968468</td>\n",
       "      <td>0.989428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.160593</td>\n",
       "      <td>0.958716</td>\n",
       "      <td>0.959551</td>\n",
       "      <td>0.957399</td>\n",
       "      <td>0.961712</td>\n",
       "      <td>0.988962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.183409</td>\n",
       "      <td>0.959862</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.959551</td>\n",
       "      <td>0.961712</td>\n",
       "      <td>0.988731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training Finished in 5390.01 seconds. - Falcon-1B_FTT_SST-2.ipynb:71\n",
      "Saving model to ./Falcon_1B_SST2_Results... - Falcon-1B_FTT_SST-2.ipynb:74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./Falcon_1B_SST2_Results\\\\tokenizer_config.json',\n",
       " './Falcon_1B_SST2_Results\\\\special_tokens_map.json',\n",
       " './Falcon_1B_SST2_Results\\\\vocab.json',\n",
       " './Falcon_1B_SST2_Results\\\\merges.txt',\n",
       " './Falcon_1B_SST2_Results\\\\added_tokens.json',\n",
       " './Falcon_1B_SST2_Results\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Phần 4: Training Config & Execution\n",
    "\n",
    "# 1. Định nghĩa Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "\n",
    "    probs = softmax(logits, axis=1)[:, 1]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(labels, probs)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall, 'roc_auc': roc_auc}\n",
    "\n",
    "# 2. Training Arguments \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_falcon_sst2',\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # --- CẤU HÌNH TỐI ƯU VRAM 24GB ---\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,    \n",
    "    per_device_eval_batch_size=32,\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_falcon',\n",
    "    logging_steps=50,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # --- QUAN TRỌNG CHO A5000 ---\n",
    "    fp16=False,     # Tắt FP16 để tránh lỗi unscale\n",
    "    bf16=True,      # Bật BF16 cho dòng Ampere\n",
    "    \n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    dataloader_num_workers=0 # Windows fix\n",
    ")\n",
    "\n",
    "# 3. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# 4. Start Training\n",
    "print(\"Starting Training (Falcon1B on SST2 | Turbo Mode BF16) - Falcon-1B_FTT_SST-2.ipynb:65\")\n",
    "start_train_time = time.time()\n",
    "trainer.train()\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "\n",
    "print(f\"\\n>>> Training Finished in {training_time:.2f} seconds. - Falcon-1B_FTT_SST-2.ipynb:71\")\n",
    "\n",
    "# 5. Save Model\n",
    "print(f\"Saving model to {SAVE_PATH}... - Falcon-1B_FTT_SST-2.ipynb:74\")\n",
    "trainer.save_model(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c649b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Final Evaluation (SST2 Test Set) - Falcon-1B_FTT_SST-2.ipynb:11\n",
      "\n",
      "====== REPORT: Falcon1B + Prompt Tuning (SST2) ====== - Falcon-1B_FTT_SST-2.ipynb:70\n",
      "1. Classification Metrics: - Falcon-1B_FTT_SST-2.ipynb:71\n",
      "Accuracy:  0.9587 - Falcon-1B_FTT_SST-2.ipynb:72\n",
      "Precision: 0.9595 - Falcon-1B_FTT_SST-2.ipynb:73\n",
      "Recall:    0.9595 - Falcon-1B_FTT_SST-2.ipynb:74\n",
      "F1Score:  0.9595 - Falcon-1B_FTT_SST-2.ipynb:75\n",
      "ROCAUC:   0.9887 - Falcon-1B_FTT_SST-2.ipynb:76\n",
      "\n",
      "2. Efficiency Metrics: - Falcon-1B_FTT_SST-2.ipynb:78\n",
      "Training Time:      5390.01 s - Falcon-1B_FTT_SST-2.ipynb:79\n",
      "Inference Latency:  10.5780 ms/sample - Falcon-1B_FTT_SST-2.ipynb:80\n",
      "Adapter Size (Disk): 2501.77 MB - Falcon-1B_FTT_SST-2.ipynb:81\n",
      "Peak RAM Usage:     1773.18 MB - Falcon-1B_FTT_SST-2.ipynb:82\n",
      "Peak VRAM Usage:    12523.73 MB - Falcon-1B_FTT_SST-2.ipynb:83\n",
      "\n",
      "Report saved to Drive: ./Falcon_1B_SST2_Results\\falcon_sst2_adapters_report.csv - Falcon-1B_FTT_SST-2.ipynb:106\n"
     ]
    }
   ],
   "source": [
    "# @title 5. Final Report (Full Metrics & Resources) - Falcon Compatible\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "print(\"Running Final Evaluation (SST2 Test Set) - Falcon-1B_FTT_SST-2.ipynb:11\")\n",
    "\n",
    "# 1. Prediction\n",
    "start_pred = time.time()\n",
    "predictions_output = trainer.predict(dataset_val)\n",
    "end_pred = time.time()\n",
    "\n",
    "# --- TÍNH TOÁN METRICS THỦ CÔNG (Để đảm bảo không bị số 0) ---\n",
    "raw_logits = predictions_output.predictions\n",
    "if isinstance(raw_logits, tuple): raw_logits = raw_logits[0]\n",
    "true_labels = predictions_output.label_ids\n",
    "\n",
    "# Chuyển đổi logits -> nhãn và xác suất\n",
    "pred_labels = np.argmax(raw_logits, axis=-1)\n",
    "pred_probs = softmax(raw_logits, axis=1)[:, 1]\n",
    "\n",
    "# Tính toán chuẩn xác bằng Sklearn\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "roc_auc = roc_auc_score(true_labels, pred_probs)\n",
    "\n",
    "# Gán ngược vào dictionary 'metrics' để code in bên dưới hoạt động đúng\n",
    "metrics = {\n",
    "    'test_accuracy': acc,\n",
    "    'test_precision': precision,\n",
    "    'test_recall': recall,\n",
    "    'test_f1': f1,\n",
    "    'test_roc_auc': roc_auc\n",
    "}\n",
    "\n",
    "total_samples = len(dataset_val)\n",
    "latency = ((end_pred - start_pred) / total_samples) * 1000 # ms/sample\n",
    "\n",
    "# 2. Save & Calculate Size (Sửa cho Falcon/PEFT)\n",
    "# Falcon Prompt Tuning dùng trainer.save_model hoặc model.save_pretrained\n",
    "trainer.save_model(SAVE_PATH) \n",
    "\n",
    "# Kiểm tra các file có thể được sinh ra bởi Falcon/PEFT\n",
    "possible_files = ['adapter_model.bin', 'adapter_model.safetensors', 'pytorch_model.bin', 'model.safetensors']\n",
    "model_size_mb = 0\n",
    "\n",
    "for file_name in possible_files:\n",
    "    file_path = os.path.join(SAVE_PATH, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        size = os.path.getsize(file_path) / (1024**2)\n",
    "        model_size_mb += size\n",
    "\n",
    "# 3. Resource Usage Monitoring\n",
    "process = psutil.Process(os.getpid())\n",
    "ram_usage = process.memory_info().rss / (1024 ** 2)\n",
    "vram_usage = torch.cuda.max_memory_allocated() / (1024 ** 2) if torch.cuda.is_available() else 0\n",
    "\n",
    "# Xử lý biến training_time (nếu chưa có thì gán = 0)\n",
    "try:\n",
    "    training_time = training_time\n",
    "except NameError:\n",
    "    training_time = 0.0\n",
    "\n",
    "# 4. Final Detailed Report (Y hệt mẫu bạn gửi)\n",
    "print(\"\\n====== REPORT: Falcon1B + Prompt Tuning (SST2) ====== - Falcon-1B_FTT_SST-2.ipynb:70\")\n",
    "print(f\"1. Classification Metrics: - Falcon-1B_FTT_SST-2.ipynb:71\")\n",
    "print(f\"Accuracy:  {metrics.get('test_accuracy', 0):.4f} - Falcon-1B_FTT_SST-2.ipynb:72\")\n",
    "print(f\"Precision: {metrics.get('test_precision', 0):.4f} - Falcon-1B_FTT_SST-2.ipynb:73\")\n",
    "print(f\"Recall:    {metrics.get('test_recall', 0):.4f} - Falcon-1B_FTT_SST-2.ipynb:74\")\n",
    "print(f\"F1Score:  {metrics.get('test_f1', 0):.4f} - Falcon-1B_FTT_SST-2.ipynb:75\")\n",
    "print(f\"ROCAUC:   {metrics.get('test_roc_auc', 0):.4f} - Falcon-1B_FTT_SST-2.ipynb:76\")\n",
    "\n",
    "print(f\"\\n2. Efficiency Metrics: - Falcon-1B_FTT_SST-2.ipynb:78\")\n",
    "print(f\"Training Time:      {training_time:.2f} s - Falcon-1B_FTT_SST-2.ipynb:79\")\n",
    "print(f\"Inference Latency:  {latency:.4f} ms/sample - Falcon-1B_FTT_SST-2.ipynb:80\")\n",
    "print(f\"Adapter Size (Disk): {model_size_mb:.2f} MB - Falcon-1B_FTT_SST-2.ipynb:81\")\n",
    "print(f\"Peak RAM Usage:     {ram_usage:.2f} MB - Falcon-1B_FTT_SST-2.ipynb:82\")\n",
    "print(f\"Peak VRAM Usage:    {vram_usage:.2f} MB - Falcon-1B_FTT_SST-2.ipynb:83\")\n",
    "\n",
    "# 5. Save CSV\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC-AUC\",\n",
    "               \"Training Time (s)\", \"Inference Latency (ms)\", \"Adapter Size (MB)\",\n",
    "               \"Peak RAM (MB)\", \"Peak VRAM (MB)\"],\n",
    "    \"Value\": [\n",
    "        metrics.get('test_accuracy', 0),\n",
    "        metrics.get('test_precision', 0),\n",
    "        metrics.get('test_recall', 0),\n",
    "        metrics.get('test_f1', 0),\n",
    "        metrics.get('test_roc_auc', 0),\n",
    "        training_time,\n",
    "        latency,\n",
    "        model_size_mb,\n",
    "        ram_usage,\n",
    "        vram_usage\n",
    "    ]\n",
    "})\n",
    "\n",
    "results_file = os.path.join(SAVE_PATH, 'falcon_sst2_adapters_report.csv')\n",
    "results_df.to_csv(results_file, index=False)\n",
    "print(f\"\\nReport saved to Drive: {results_file} - Falcon-1B_FTT_SST-2.ipynb:106\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
