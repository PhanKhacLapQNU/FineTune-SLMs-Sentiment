{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22774,"status":"ok","timestamp":1766813862079,"user":{"displayName":"ngo cong","userId":"09373141506726908581"},"user_tz":-420},"id":"AvWtAB6uHTa_","outputId":"e3e9d884-3e56-4d9a-9933-69bf8a74a19d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using device: cuda\n","--- Loading SST-2 ---\n"]}],"source":["# @title 1. Setup Environment & Load SST-2\n","!pip install -q transformers peft datasets evaluate scikit-learn accelerate psutil\n","\n","import os\n","import time\n","import psutil\n","import torch\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback\n",")\n","from peft import get_peft_model, LoraConfig, TaskType\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n","from scipy.special import softmax\n","\n","# 1. Mount Drive\n","drive.mount('/content/drive')\n","SAVE_PATH = '/content/drive/My Drive/SLM_Research/SST2_Falcon1B_LoRA'\n","if not os.path.exists(SAVE_PATH):\n","    os.makedirs(SAVE_PATH)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# 2. Load SST-2\n","print(\"--- Loading SST-2 ---\")\n","dataset = load_dataset(\"glue\", \"sst2\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["768c8f1ea6a041feb550876d0e054a3a","ce10bcd278c84cdda49a823e88c14236","6d48375d499648289ab1f876f458867d","e8e04a67b97645c38a9b0b88ad4eee1f","4be5afc64de14feb90ac4c3c52aaa42d","5bf3694d7b4440fdbfcfa4a3a1b80e75","242f16cd86e146049de53442c8b62677","c4692baec0a74ad38c5b18b9455f3007","4333ef2738ae4e399a81718e38b78618","c4b27235824e4afeb3d4851bd8ed8694","9f14e0f883194190a1d639ac00f76a3c"]},"executionInfo":{"elapsed":2774,"status":"ok","timestamp":1766813864898,"user":{"displayName":"ngo cong","userId":"09373141506726908581"},"user_tz":-420},"id":"ofjJTJ2tHdXm","outputId":"b348c6df-be2e-414b-b4c5-52240f91ab90"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Tokenizing Dataset (Falcon) ---\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/872 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768c8f1ea6a041feb550876d0e054a3a"}},"metadata":{}}],"source":["# @title 2. Tokenization\n","MODEL_NAME = 'tiiuae/falcon-rw-1b'\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","# Thi·∫øt l·∫≠p PAD Token b·∫±ng EOS Token (B·∫Øt bu·ªôc cho Falcon)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=64)\n","\n","print(\"--- Tokenizing Dataset (Falcon) ---\")\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","\n","tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","tokenized_datasets.set_format(\"torch\")\n","\n","dataset_train = tokenized_datasets[\"train\"]\n","dataset_val = tokenized_datasets[\"validation\"]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6256,"status":"ok","timestamp":1766814038807,"user":{"displayName":"ngo cong","userId":"09373141506726908581"},"user_tz":-420},"id":"GF_nVdgKHeSL","outputId":"27fdab3d-73cc-4de1-df41-e433f0cb50f2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-rw-1b and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["--- Casting trainable parameters to Float32 for stability ---\n","trainable params: 2,363,392 || all params: 1,313,992,704 || trainable%: 0.1799\n"]}],"source":["# @title 3. Falcon-1B + LoRA Configuration (Fixed FP16 Error)\n","\n","# 1. Load Base Model (FP16)\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels=2,\n","    torch_dtype=torch.float16 # Load model g·ªëc nh·∫π h∆°n\n",")\n","model.config.pad_token_id = tokenizer.pad_token_id\n","model.to(device)\n","\n","# 2. ƒê·ªãnh nghƒ©a LoRA Config\n","lora_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS,\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"query_key_value\", \"dense\"],\n","    lora_dropout=0.1,\n","    bias=\"none\"\n",")\n","\n","# 3. Ch√®n LoRA\n","model = get_peft_model(model, lora_config)\n","\n","# --- KH·∫ÆC PH·ª§C L·ªñI VALUE ERROR ---\n","# √âp to√†n b·ªô c√°c tham s·ªë c·∫ßn hu·∫•n luy·ªán (LoRA adapters) v·ªÅ float32\n","# ƒêi·ªÅu n√†y gi√∫p Scaler t√≠nh to√°n gradient ch√≠nh x√°c, tr√°nh l·ªói unscale FP16\n","print(\"--- Casting trainable parameters to Float32 for stability ---\")\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        param.data = param.data.to(torch.float32)\n","\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"id":"8AB1gRVSHhDQ","executionInfo":{"status":"ok","timestamp":1766815874045,"user_tz":-420,"elapsed":1816925,"user":{"displayName":"ngo cong","userId":"09373141506726908581"}},"outputId":"03b22e43-fed4-47e0-a764-d8b878c03dd5"},"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50256, 'bos_token_id': 50256}.\n"]},{"output_type":"stream","name":"stdout","text":["--- Ki·ªÉm tra checkpoint t·∫°i: /content/drive/My Drive/SLM_Research/SST2_Falcon1B_LoRA ---\n","üîÑ T√¨m th·∫•y checkpoint: /content/drive/My Drive/SLM_Research/SST2_Falcon1B_LoRA/checkpoint-4000\n","üöÄ ƒêang kh√¥i ph·ª•c v√† ch·∫°y ti·∫øp...\n"]},{"output_type":"stream","name":"stderr","text":["Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n","\tlogging_steps: 50 (from args) != 100 (from trainer_state.json)\n","\tsave_steps: 500 (from args) != 1000 (from trainer_state.json)\n","/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12630/12630 30:13, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Roc Auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.161500</td>\n","      <td>0.170092</td>\n","      <td>0.951835</td>\n","      <td>0.952381</td>\n","      <td>0.958904</td>\n","      <td>0.945946</td>\n","      <td>0.983629</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.149200</td>\n","      <td>0.172371</td>\n","      <td>0.947248</td>\n","      <td>0.948315</td>\n","      <td>0.946188</td>\n","      <td>0.950450</td>\n","      <td>0.984153</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.163500</td>\n","      <td>0.170795</td>\n","      <td>0.950688</td>\n","      <td>0.951192</td>\n","      <td>0.958810</td>\n","      <td>0.943694</td>\n","      <td>0.984111</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t v√† ƒë√£ l∆∞u!\n"]}],"source":["# @title 4. Smart Training (Falcon LoRA) - Fix NameError\n","import os\n","import time\n","import numpy as np\n","import torch\n","from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n","from transformers.trainer_utils import get_last_checkpoint\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n","from scipy.special import softmax\n","\n","# 1. ƒê·ªãnh nghƒ©a Metrics (ƒê·ªãnh nghƒ©a l·∫°i t·∫°i ƒë√¢y ƒë·ªÉ tr√°nh l·ªói NameError)\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    # Falcon c√≥ th·ªÉ tr·∫£ v·ªÅ tuple, c·∫ßn l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n\n","    if isinstance(logits, tuple):\n","        logits = logits[0]\n","\n","    predictions = np.argmax(logits, axis=-1)\n","    probs = softmax(logits, axis=1)[:, 1]\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n","    acc = accuracy_score(labels, predictions)\n","    roc_auc = roc_auc_score(labels, probs)\n","\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'roc_auc': roc_auc\n","    }\n","\n","# 2. Training Arguments (T·ªëi ∆∞u VRAM cho Falcon)\n","training_args = TrainingArguments(\n","    output_dir=SAVE_PATH,\n","    num_train_epochs=3,\n","\n","    # --- C·∫•u h√¨nh Ti·∫øt ki·ªám VRAM ---\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    # -------------------------------\n","\n","    # --- C·∫•u h√¨nh Checkpoint theo Epoch ---\n","    save_strategy=\"epoch\",\n","\n","    save_total_limit=1,\n","\n","    eval_strategy=\"epoch\",\n","\n","\n","    learning_rate=5e-5,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_steps=50,\n","\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","\n","    fp16=True,\n","    report_to=\"none\"\n",")\n","\n","# 3. Kh·ªüi t·∫°o Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset_train,\n","    eval_dataset=dataset_val,\n","    processing_class=tokenizer,\n","    compute_metrics=compute_metrics,  # ƒê√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü tr√™n\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",")\n","\n","# 4. T·ª± ƒë·ªông Resume (Ch·∫°y ti·∫øp n·∫øu c√≥ checkpoint c≈©)\n","# T·∫Øt cache ƒë·ªÉ tr√°nh xung ƒë·ªôt v·ªõi gradient checkpointing\n","model.config.use_cache = False\n","\n","print(f\"--- Ki·ªÉm tra checkpoint t·∫°i: {SAVE_PATH} ---\")\n","last_checkpoint = get_last_checkpoint(SAVE_PATH)\n","\n","start_train_time = time.time()\n","\n","if last_checkpoint:\n","    print(f\"üîÑ T√¨m th·∫•y checkpoint: {last_checkpoint}\")\n","    print(\"üöÄ ƒêang kh√¥i ph·ª•c v√† ch·∫°y ti·∫øp...\")\n","    trainer.train(resume_from_checkpoint=last_checkpoint)\n","else:\n","    print(\"‚ú® Kh√¥ng th·∫•y checkpoint c≈©. B·∫Øt ƒë·∫ßu train m·ªõi...\")\n","    trainer.train()\n","\n","training_time = time.time() - start_train_time\n","\n","# 5. L∆∞u model cu·ªëi c√πng\n","trainer.save_model(SAVE_PATH)\n","print(\"‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t v√† ƒë√£ l∆∞u!\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"abkQeuD9Hkph","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1766815882614,"user_tz":-420,"elapsed":8557,"user":{"displayName":"ngo cong","userId":"09373141506726908581"}},"outputId":"d5324285-0b98-4e38-f4d3-daad3e4e80ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Running Final Evaluation on Validation Set ---\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","====== REPORT: Falcon-1B + LoRA on SST-2 ======\n","1. Classification Metrics:\n","   - Accuracy:  0.9518\n","   - Precision: 0.9589\n","   - Recall:    0.9459\n","   - F1-Score:  0.9524\n","   - ROC-AUC:   0.9836\n","\n","2. Efficiency Metrics:\n","   - Training Time:      1816.08 s\n","   - Inference Latency:  9.7882 ms/sample\n","   - Adapter Size (Disk): 9.0287 MB\n","   - Peak RAM Usage:     3713.37 MB\n","   - Peak VRAM Usage:    5064.85 MB\n","\n","Report saved to /content/drive/My Drive/SLM_Research/SST2_Falcon1B_LoRA/sst2_falcon1b_lora_results.csv\n"]}],"source":["# @title 5. Final Evaluation (Full Metrics & Resources)\n","import os\n","import time\n","import psutil\n","import torch\n","import pandas as pd\n","import numpy as np\n","\n","print(\"--- Running Final Evaluation on Validation Set ---\")\n","\n","# 1. Prediction & Latency\n","start_pred_time = time.time()\n","predictions_output = trainer.predict(dataset_val)\n","end_pred_time = time.time()\n","\n","# L·∫•y metrics\n","metrics = predictions_output.metrics\n","total_samples = len(dataset_val)\n","latency = ((end_pred_time - start_pred_time) / total_samples) * 1000 # ms/sample\n","\n","# 2. Model Size Check (Adapter Only)\n","adapter_file = os.path.join(SAVE_PATH, 'adapter_model.safetensors')\n","if not os.path.exists(adapter_file):\n","    adapter_file = os.path.join(SAVE_PATH, 'adapter_model.bin')\n","\n","adapter_size = 0\n","if os.path.exists(adapter_file):\n","    adapter_size = os.path.getsize(adapter_file) / (1024 * 1024)\n","\n","# 3. Resource Usage\n","process = psutil.Process(os.getpid())\n","ram_usage = process.memory_info().rss / (1024 ** 2)\n","# L·∫•y Peak VRAM\n","vram_usage = torch.cuda.max_memory_allocated() / (1024 ** 2) if torch.cuda.is_available() else 0\n","\n","# 4. Final Detailed Report\n","print(\"\\n====== REPORT: Falcon-1B + LoRA on SST-2 ======\")\n","print(f\"1. Classification Metrics:\")\n","print(f\"   - Accuracy:  {metrics.get('test_accuracy', 0):.4f}\")\n","print(f\"   - Precision: {metrics.get('test_precision', 0):.4f}\")\n","print(f\"   - Recall:    {metrics.get('test_recall', 0):.4f}\")\n","print(f\"   - F1-Score:  {metrics.get('test_f1', 0):.4f}\")\n","print(f\"   - ROC-AUC:   {metrics.get('test_roc_auc', 0):.4f}\")\n","\n","print(f\"\\n2. Efficiency Metrics:\")\n","print(f\"   - Training Time:      {training_time:.2f} s\")\n","print(f\"   - Inference Latency:  {latency:.4f} ms/sample\")\n","print(f\"   - Adapter Size (Disk): {adapter_size:.4f} MB\")\n","print(f\"   - Peak RAM Usage:     {ram_usage:.2f} MB\")\n","print(f\"   - Peak VRAM Usage:    {vram_usage:.2f} MB\")\n","\n","# 5. Save CSV\n","results_df = pd.DataFrame({\n","    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC-AUC\",\n","               \"Training Time (s)\", \"Inference Latency (ms)\", \"Adapter Size (MB)\",\n","               \"Peak RAM (MB)\", \"Peak VRAM (MB)\"],\n","    \"Value\": [\n","        metrics.get('test_accuracy', 0),\n","        metrics.get('test_precision', 0),\n","        metrics.get('test_recall', 0),\n","        metrics.get('test_f1', 0),\n","        metrics.get('test_roc_auc', 0),\n","        training_time,\n","        latency,\n","        adapter_size,\n","        ram_usage,\n","        vram_usage\n","    ]\n","})\n","\n","results_file = os.path.join(SAVE_PATH, 'sst2_falcon1b_lora_results.csv')\n","results_df.to_csv(results_file, index=False)\n","print(f\"\\nReport saved to {results_file}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOOVljQbfkVRo8kq4/KYd3n"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"768c8f1ea6a041feb550876d0e054a3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce10bcd278c84cdda49a823e88c14236","IPY_MODEL_6d48375d499648289ab1f876f458867d","IPY_MODEL_e8e04a67b97645c38a9b0b88ad4eee1f"],"layout":"IPY_MODEL_4be5afc64de14feb90ac4c3c52aaa42d"}},"ce10bcd278c84cdda49a823e88c14236":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bf3694d7b4440fdbfcfa4a3a1b80e75","placeholder":"‚Äã","style":"IPY_MODEL_242f16cd86e146049de53442c8b62677","value":"Map:‚Äá100%"}},"6d48375d499648289ab1f876f458867d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4692baec0a74ad38c5b18b9455f3007","max":872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4333ef2738ae4e399a81718e38b78618","value":872}},"e8e04a67b97645c38a9b0b88ad4eee1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4b27235824e4afeb3d4851bd8ed8694","placeholder":"‚Äã","style":"IPY_MODEL_9f14e0f883194190a1d639ac00f76a3c","value":"‚Äá872/872‚Äá[00:00&lt;00:00,‚Äá2006.28‚Äáexamples/s]"}},"4be5afc64de14feb90ac4c3c52aaa42d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bf3694d7b4440fdbfcfa4a3a1b80e75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"242f16cd86e146049de53442c8b62677":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4692baec0a74ad38c5b18b9455f3007":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4333ef2738ae4e399a81718e38b78618":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4b27235824e4afeb3d4851bd8ed8694":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f14e0f883194190a1d639ac00f76a3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}