{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63ce21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Thi·∫øt b·ªã: cuda - Falcon-1B_LoRA_IMDB.ipynb:31\n",
      "üî• GPU: NVIDIA RTX A5000 - Falcon-1B_LoRA_IMDB.ipynb:33\n",
      "\n",
      " ƒêang t·∫£i d·ªØ li·ªáu IMDB - Falcon-1B_LoRA_IMDB.ipynb:39\n",
      "ƒêang t·∫£i Tokenizer: tiiuae/falcon-rw-1b - Falcon-1B_LoRA_IMDB.ipynb:42\n",
      "Tokenizing (ƒê·ª£i x√≠u)... - Falcon-1B_LoRA_IMDB.ipynb:53\n",
      "\n",
      " ƒêang t·∫£i Model: tiiuae/falcon-rw-1b - Falcon-1B_LoRA_IMDB.ipynb:65\n",
      "> Ch·∫ø ƒë·ªô Precision: torch.bfloat16 - Falcon-1B_LoRA_IMDB.ipynb:69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-rw-1b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√†i ƒë·∫∑t LoRA - Falcon-1B_LoRA_IMDB.ipynb:82\n",
      "trainable params: 2,363,392 || all params: 1,313,992,704 || trainable%: 0.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50256, 'bos_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán T·ªêI ∆ØU (Batch=32, Checkpoint=False) - Falcon-1B_LoRA_IMDB.ipynb:170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1173/1173 2:16:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.127506</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>0.955877</td>\n",
       "      <td>0.933638</td>\n",
       "      <td>0.979200</td>\n",
       "      <td>0.991577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.106698</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>0.954581</td>\n",
       "      <td>0.970160</td>\n",
       "      <td>0.991927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.120093</td>\n",
       "      <td>0.963240</td>\n",
       "      <td>0.963244</td>\n",
       "      <td>0.963129</td>\n",
       "      <td>0.963360</td>\n",
       "      <td>0.992377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TTNT1\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "c:\\Users\\TTNT1\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "c:\\Users\\TTNT1\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "c:\\Users\\TTNT1\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ƒê√°nh gi√° m√¥ h√¨nh - Falcon-1B_LoRA_IMDB.ipynb:183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TTNT1\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Falcon-1B_LoRA_IMDB.ipynb:207==================================================\n",
      "====== REPORT: Falcon1B + LoRA (A5000 Optimized) ====== - Falcon-1B_LoRA_IMDB.ipynb:208\n",
      "= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209= - Falcon-1B_LoRA_IMDB.ipynb:209\n",
      "1. Classification Metrics: - Falcon-1B_LoRA_IMDB.ipynb:210\n",
      "Accuracy:   0.9632 - Falcon-1B_LoRA_IMDB.ipynb:211\n",
      "Precision:  0.9631 - Falcon-1B_LoRA_IMDB.ipynb:212\n",
      "Recall:     0.9634 - Falcon-1B_LoRA_IMDB.ipynb:213\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'F1Score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 232\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ K·∫øt qu·∫£ ƒë√£ l∆∞u t·∫°i: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_PATH,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_report_optimized.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Falcon-1B_LoRA_IMDB.ipynb:229\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 232\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 214\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Falcon-1B_LoRA_IMDB.ipynb:212\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall:     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Falcon-1B_LoRA_IMDB.ipynb:213\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1Score:    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF1Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Falcon-1B_LoRA_IMDB.ipynb:214\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROCAUC:     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROCAUC\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Falcon-1B_LoRA_IMDB.ipynb:215\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. Efficiency Metrics: - Falcon-1B_LoRA_IMDB.ipynb:217\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'F1Score'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG\n",
    "# ==============================================================================\n",
    "MODEL_NAME = \"tiiuae/falcon-rw-1b\"\n",
    "SAVE_PATH = \"./results_falcon_imdb_lora_optimized\" # ƒê·ªïi t√™n folder ƒë·ªÉ tr√°nh ghi ƒë√®\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Thi·∫øt b·ªã: {device} - Falcon-1B_LoRA_IMDB.ipynb:31\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• GPU: {torch.cuda.get_device_name(0)} - Falcon-1B_LoRA_IMDB.ipynb:33\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. X·ª¨ L√ù D·ªÆ LI·ªÜU\n",
    "# ==============================================================================\n",
    "def load_data():\n",
    "    print(\"\\n ƒêang t·∫£i d·ªØ li·ªáu IMDB - Falcon-1B_LoRA_IMDB.ipynb:39\")\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    \n",
    "    print(f\"ƒêang t·∫£i Tokenizer: {MODEL_NAME} - Falcon-1B_LoRA_IMDB.ipynb:42\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    print(\"Tokenizing (ƒê·ª£i x√≠u)... - Falcon-1B_LoRA_IMDB.ipynb:53\")\n",
    "    tokenized = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    tokenized = tokenized.remove_columns([\"text\"])\n",
    "    tokenized.set_format(\"torch\") \n",
    "\n",
    "    return tokenized[\"train\"], tokenized[\"test\"], tokenizer\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MODEL & LORA \n",
    "# ==============================================================================\n",
    "def setup_model(tokenizer):\n",
    "    print(f\"\\n ƒêang t·∫£i Model: {MODEL_NAME} - Falcon-1B_LoRA_IMDB.ipynb:65\")\n",
    "    \n",
    "    is_bf16_supported = torch.cuda.is_bf16_supported()\n",
    "    dtype = torch.bfloat16 if is_bf16_supported else torch.float16\n",
    "    print(f\"> Ch·∫ø ƒë·ªô Precision: {dtype} - Falcon-1B_LoRA_IMDB.ipynb:69\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        torch_dtype=dtype,\n",
    "        use_safetensors=True \n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False \n",
    "    \n",
    "    model.gradient_checkpointing_disable() \n",
    "\n",
    "    print(\"C√†i ƒë·∫∑t LoRA - Falcon-1B_LoRA_IMDB.ipynb:82\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query_key_value\", \"dense\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. METRICS\n",
    "# ==============================================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "        \n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    probs = softmax(logits, axis=1)[:, 1]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    try: roc_auc = roc_auc_score(labels, probs)\n",
    "    except: roc_auc = 0.0\n",
    "        \n",
    "    return {\n",
    "        'accuracy': acc, \n",
    "        'f1': f1, \n",
    "        'precision': precision, \n",
    "        'recall': recall, \n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. MAIN\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    train_data, eval_data, tokenizer = load_data()\n",
    "    model = setup_model(tokenizer)\n",
    "    \n",
    "    use_bf16 = torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = not use_bf16\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=SAVE_PATH,\n",
    "        num_train_epochs=3,\n",
    "        \n",
    "        per_device_train_batch_size=16, \n",
    "        gradient_accumulation_steps=2,\n",
    "        \n",
    "        gradient_checkpointing=False,\n",
    "        \n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        \n",
    "        learning_rate=2e-4, \n",
    "        warmup_steps=100,\n",
    "        logging_steps=50,\n",
    "        \n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        \n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=0 \n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    print(f\"\\n B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán T·ªêI ∆ØU (Batch=32, Checkpoint=False) - Falcon-1B_LoRA_IMDB.ipynb:170\")\n",
    "    last_ckpt = get_last_checkpoint(SAVE_PATH)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if last_ckpt:\n",
    "        print(f\"üîÑ T√¨m th·∫•y checkpoint c≈©: {last_ckpt}. Ti·∫øp t·ª•c train... - Falcon-1B_LoRA_IMDB.ipynb:175\")\n",
    "        trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # --- Eval ---\n",
    "    print(\"\\n ƒê√°nh gi√° m√¥ h√¨nh - Falcon-1B_LoRA_IMDB.ipynb:183\")\n",
    "    start_pred = time.time()\n",
    "    preds = trainer.predict(eval_data)\n",
    "    end_pred = time.time()\n",
    "    \n",
    "    m = preds.metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": m.get(\"test_accuracy\", 0),\n",
    "        \"Precision\": m.get(\"test_precision\", 0),\n",
    "        \"Recall\": m.get(\"test_recall\", 0),\n",
    "        \"F1-Score\": m.get(\"test_f1\", 0),\n",
    "        \"ROC-AUC\": m.get(\"test_roc_auc\", 0)\n",
    "    }\n",
    "    \n",
    "    latency = ((end_pred - start_pred) / len(eval_data)) * 1000\n",
    "    \n",
    "    trainer.save_model(SAVE_PATH)\n",
    "    adapter_path = os.path.join(SAVE_PATH, \"adapter_model.safetensors\")\n",
    "    if not os.path.exists(adapter_path):\n",
    "        adapter_path = os.path.join(SAVE_PATH, \"adapter_model.bin\")\n",
    "    size_mb = os.path.getsize(adapter_path) / (1024**2) if os.path.exists(adapter_path) else 0\n",
    "    \n",
    "    vram_mb = torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else 0\n",
    "\n",
    "    print(\"\\n - Falcon-1B_LoRA_IMDB.ipynb:207\" + \"=\"*50)\n",
    "    print(\"====== REPORT: Falcon1B + LoRA (A5000 Optimized) ====== - Falcon-1B_LoRA_IMDB.ipynb:208\")\n",
    "    print(\"= - Falcon-1B_LoRA_IMDB.ipynb:209\"*50)\n",
    "    print(\"1. Classification Metrics: - Falcon-1B_LoRA_IMDB.ipynb:210\")\n",
    "    print(f\"Accuracy:   {metrics['Accuracy']:.4f} - Falcon-1B_LoRA_IMDB.ipynb:211\")\n",
    "    print(f\"Precision:  {metrics['Precision']:.4f} - Falcon-1B_LoRA_IMDB.ipynb:212\")\n",
    "    print(f\"Recall:     {metrics['Recall']:.4f} - Falcon-1B_LoRA_IMDB.ipynb:213\")\n",
    "    print(f\"F1Score:    {metrics['F1Score']:.4f} - Falcon-1B_LoRA_IMDB.ipynb:214\")\n",
    "    print(f\"ROCAUC:     {metrics['ROCAUC']:.4f} - Falcon-1B_LoRA_IMDB.ipynb:215\")\n",
    "    \n",
    "    print(\"\\n2. Efficiency Metrics: - Falcon-1B_LoRA_IMDB.ipynb:217\")\n",
    "    print(f\"Training Time:     {train_time:.2f} s - Falcon-1B_LoRA_IMDB.ipynb:218\")\n",
    "    print(f\"Inference Latency: {latency:.4f} ms/sample - Falcon-1B_LoRA_IMDB.ipynb:219\")\n",
    "    print(f\"Adapter Size:      {size_mb:.2f} MB - Falcon-1B_LoRA_IMDB.ipynb:220\")\n",
    "    print(f\"Peak VRAM:         {vram_mb:.2f} MB - Falcon-1B_LoRA_IMDB.ipynb:221\")\n",
    "    print(\"= - Falcon-1B_LoRA_IMDB.ipynb:222\"*50)\n",
    "\n",
    "    df_data = {\n",
    "        \"Metric\": list(metrics.keys()) + [\"Training Time (s)\", \"Latency (ms)\", \"Size (MB)\", \"VRAM (MB)\"],\n",
    "        \"Value\": list(metrics.values()) + [train_time, latency, size_mb, vram_mb]\n",
    "    }\n",
    "    pd.DataFrame(df_data).to_csv(os.path.join(SAVE_PATH, \"final_report_optimized.csv\"), index=False)\n",
    "    print(f\"‚úÖ K·∫øt qu·∫£ ƒë√£ l∆∞u t·∫°i: {os.path.join(SAVE_PATH, 'final_report_optimized.csv')} - Falcon-1B_LoRA_IMDB.ipynb:229\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543bf6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• ƒêang chu·∫©n b·ªã Tokenizer v√† D·ªØ li·ªáu Test... - Falcon-1B_LoRA_IMDB.ipynb:27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:09<00:00, 2687.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è ƒêang t·∫£i Model v√† LoRA Adapter... - Falcon-1B_LoRA_IMDB.ipynb:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-rw-1b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ƒêang ƒë√°nh gi√° 25,000 m·∫´u t·∫≠p Test (Vui l√≤ng ƒë·ª£i)... - Falcon-1B_LoRA_IMDB.ipynb:66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TTNT1\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Falcon-1B_LoRA_IMDB.ipynb:106============================================================\n",
      "üìä FINAL REPORT: Falcon1B + LoRA (IMDB)  Full 10 Metrics - Falcon-1B_LoRA_IMDB.ipynb:107\n",
      "= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108= - Falcon-1B_LoRA_IMDB.ipynb:108\n",
      "1. Classification Metrics: - Falcon-1B_LoRA_IMDB.ipynb:109\n",
      "Accuracy:          0.9631 - Falcon-1B_LoRA_IMDB.ipynb:110\n",
      "Precision:         0.9629 - Falcon-1B_LoRA_IMDB.ipynb:111\n",
      "Recall:            0.9634 - Falcon-1B_LoRA_IMDB.ipynb:112\n",
      "F1Score:          0.9631 - Falcon-1B_LoRA_IMDB.ipynb:113\n",
      "ROCAUC:           0.9924 - Falcon-1B_LoRA_IMDB.ipynb:114\n",
      "\n",
      "2. Efficiency Metrics: - Falcon-1B_LoRA_IMDB.ipynb:116\n",
      "Training Time:      0.00 s - Falcon-1B_LoRA_IMDB.ipynb:117\n",
      "Inference Latency:  48.7468 ms/sample - Falcon-1B_LoRA_IMDB.ipynb:118\n",
      "Adapter Size (Disk): 9.02 MB - Falcon-1B_LoRA_IMDB.ipynb:119\n",
      "Peak RAM Usage:     1621.91 MB - Falcon-1B_LoRA_IMDB.ipynb:120\n",
      "Peak VRAM Usage:    4823.33 MB - Falcon-1B_LoRA_IMDB.ipynb:121\n",
      "= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122= - Falcon-1B_LoRA_IMDB.ipynb:122\n",
      "‚úÖ B√°o c√°o ƒë·∫ßy ƒë·ªß (10 ch·ªâ s·ªë) ƒë√£ l∆∞u t·∫°i: ./results_falcon_imdb_lora_optimized\\falcon_imdb_lora_full_report.csv - Falcon-1B_LoRA_IMDB.ipynb:136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === B∆Ø·ªöC 1: FIX L·ªñI B·∫¢O M·∫¨T & ƒê∆Ø·ªúNG D·∫™N ===\n",
    "import transformers.utils.hub\n",
    "os.environ[\"TRANSFORMERS_VERIFY_SCHEDULED_CVES\"] = \"False\"\n",
    "# Bypass ki·ªÉm tra phi√™n b·∫£n Torch cho CVE-2025-32434\n",
    "transformers.utils.hub.is_torch_greater_or_equal_than_2_6 = lambda: True\n",
    "\n",
    "MODEL_NAME = \"tiiuae/falcon-rw-1b\"\n",
    "# ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n n√†y kh·ªõp v·ªõi th∆∞ m·ª•c LoRA c·ªßa b·∫°n\n",
    "SAVE_PATH = \"./results_falcon_imdb_lora_optimized\" \n",
    "\n",
    "# === B∆Ø·ªöC 2: KH·ªûI T·∫†O L·∫†I (Tr√°nh l·ªói NameError) ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"üì• ƒêang chu·∫©n b·ªã Tokenizer v√† D·ªØ li·ªáu Test... - Falcon-1B_LoRA_IMDB.ipynb:27\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "dataset_test = load_dataset(\"imdb\")[\"test\"]\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_test = dataset_test.map(tokenize_fn, batched=True)\n",
    "tokenized_test.set_format(\"torch\")\n",
    "\n",
    "print(\"üèóÔ∏è ƒêang t·∫£i Model v√† LoRA Adapter... - Falcon-1B_LoRA_IMDB.ipynb:41\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=2, \n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load tr·ªçng s·ªë LoRA\n",
    "model = PeftModel.from_pretrained(base_model, SAVE_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Kh·ªüi t·∫°o Trainer v·ªõi DataCollator ƒë·ªÉ tr√°nh l·ªói Padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=\"./temp\", per_device_eval_batch_size=8, report_to=\"none\"),\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# === B∆Ø·ªöC 3: CH·∫†Y EVALUATION ===\n",
    "print(\"üöÄ ƒêang ƒë√°nh gi√° 25,000 m·∫´u t·∫≠p Test (Vui l√≤ng ƒë·ª£i)... - Falcon-1B_LoRA_IMDB.ipynb:66\")\n",
    "start_pred = time.time()\n",
    "predictions_output = trainer.predict(tokenized_test)\n",
    "end_pred = time.time()\n",
    "\n",
    "# === B∆Ø·ªöC 4: T√çNH TO√ÅN ƒê·ª¶ 10 CH·ªà S·ªê ===\n",
    "# --- 1-5: Classification Metrics ---\n",
    "raw_logits = predictions_output.predictions\n",
    "if isinstance(raw_logits, tuple): raw_logits = raw_logits[0]\n",
    "true_labels = predictions_output.label_ids\n",
    "pred_labels = np.argmax(raw_logits, axis=-1)\n",
    "pred_probs = softmax(raw_logits, axis=1)[:, 1]\n",
    "\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "roc_auc = roc_auc_score(true_labels, pred_probs)\n",
    "\n",
    "# --- 6: Training Time (N·∫øu m·∫•t session, h√£y ƒëi·ªÅn th·ªß c√¥ng gi√° tr·ªã b·∫°n ƒë√£ train) ---\n",
    "try:\n",
    "    final_training_time = train_time \n",
    "except NameError:\n",
    "    final_training_time = 0.0 # Placeholder\n",
    "\n",
    "# --- 7: Inference Latency ---\n",
    "latency = ((end_pred - start_pred) / len(tokenized_test)) * 1000 # ms/sample\n",
    "\n",
    "# --- 8: Adapter Size ---\n",
    "model_size_mb = 0\n",
    "for f in ['adapter_model.safetensors', 'adapter_model.bin']:\n",
    "    path = os.path.join(SAVE_PATH, f)\n",
    "    if os.path.exists(path):\n",
    "        model_size_mb = os.path.getsize(path) / (1024**2)\n",
    "        break\n",
    "\n",
    "# --- 9-10: Resource Usage ---\n",
    "process = psutil.Process(os.getpid())\n",
    "ram_usage = process.memory_info().rss / (1024 ** 2)\n",
    "vram_usage = torch.cuda.max_memory_allocated() / (1024 ** 2) if torch.cuda.is_available() else 0\n",
    "\n",
    "# === B∆Ø·ªöC 5: IN B√ÅO C√ÅO CHI TI·∫æT ===\n",
    "print(\"\\n - Falcon-1B_LoRA_IMDB.ipynb:106\" + \"=\"*60)\n",
    "print(\"üìä FINAL REPORT: Falcon1B + LoRA (IMDB)  Full 10 Metrics - Falcon-1B_LoRA_IMDB.ipynb:107\")\n",
    "print(\"= - Falcon-1B_LoRA_IMDB.ipynb:108\"*60)\n",
    "print(f\"1. Classification Metrics: - Falcon-1B_LoRA_IMDB.ipynb:109\")\n",
    "print(f\"Accuracy:          {acc:.4f} - Falcon-1B_LoRA_IMDB.ipynb:110\")\n",
    "print(f\"Precision:         {precision:.4f} - Falcon-1B_LoRA_IMDB.ipynb:111\")\n",
    "print(f\"Recall:            {recall:.4f} - Falcon-1B_LoRA_IMDB.ipynb:112\")\n",
    "print(f\"F1Score:          {f1:.4f} - Falcon-1B_LoRA_IMDB.ipynb:113\")\n",
    "print(f\"ROCAUC:           {roc_auc:.4f} - Falcon-1B_LoRA_IMDB.ipynb:114\")\n",
    "\n",
    "print(f\"\\n2. Efficiency Metrics: - Falcon-1B_LoRA_IMDB.ipynb:116\")\n",
    "print(f\"Training Time:      {final_training_time:.2f} s - Falcon-1B_LoRA_IMDB.ipynb:117\")\n",
    "print(f\"Inference Latency:  {latency:.4f} ms/sample - Falcon-1B_LoRA_IMDB.ipynb:118\")\n",
    "print(f\"Adapter Size (Disk): {model_size_mb:.2f} MB - Falcon-1B_LoRA_IMDB.ipynb:119\")\n",
    "print(f\"Peak RAM Usage:     {ram_usage:.2f} MB - Falcon-1B_LoRA_IMDB.ipynb:120\")\n",
    "print(f\"Peak VRAM Usage:    {vram_usage:.2f} MB - Falcon-1B_LoRA_IMDB.ipynb:121\")\n",
    "print(\"= - Falcon-1B_LoRA_IMDB.ipynb:122\"*60)\n",
    "\n",
    "# === B∆Ø·ªöC 6: L∆ØU CSV ===\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC-AUC\",\n",
    "               \"Training Time (s)\", \"Inference Latency (ms)\", \"Adapter Size (MB)\",\n",
    "               \"Peak RAM (MB)\", \"Peak VRAM (MB)\"],\n",
    "    \"Value\": [acc, precision, recall, f1, roc_auc, \n",
    "              final_training_time, latency, model_size_mb, \n",
    "              ram_usage, vram_usage]\n",
    "})\n",
    "\n",
    "results_file = os.path.join(SAVE_PATH, 'falcon_imdb_lora_full_report.csv')\n",
    "results_df.to_csv(results_file, index=False)\n",
    "print(f\"‚úÖ B√°o c√°o ƒë·∫ßy ƒë·ªß (10 ch·ªâ s·ªë) ƒë√£ l∆∞u t·∫°i: {results_file} - Falcon-1B_LoRA_IMDB.ipynb:136\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
